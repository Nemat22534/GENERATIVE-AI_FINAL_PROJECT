{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nemat22534/project_english_to_russian_translation/blob/main/Nemat_Dadashov_210208749_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KOBjUv_MMS_x"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets sacrebleu sentencepiece\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Try loading the opus_books dataset with a safer approach\n",
        "try:\n",
        "    dataset = load_dataset(\"opus_books\", \"en_ru\")\n",
        "    print(\"Dataset loaded successfully!\")\n",
        "    # Selecting the first 1000 training samples and 200 validation samples\n",
        "    train_data = dataset[\"train\"].select(range(1000))  # 1000 samples for training\n",
        "    val_data = dataset[\"validation\"].select(range(200))  # 200 samples for validation\n",
        "\n",
        "    # Check the structure of the selected subsets\n",
        "    print(f\"Training data sample size: {len(train_data)}\")\n",
        "    print(f\"Validation data sample size: {len(val_data)}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset: {e}\")\n",
        "\n",
        "\n",
        "def tokenize(examples):\n",
        "    # Extract translations (handling potential multiple translations)\n",
        "    translations = [ex[0] if isinstance(ex, list) else ex for ex in examples[\"translation\"]]\n",
        "\n",
        "    # Prepare English translations for tokenization\n",
        "    en_texts = [t[\"en\"] for t in translations]\n",
        "\n",
        "    # Prepare Russian translations for tokenization\n",
        "    ru_texts = [t[\"ru\"] for t in translations]\n",
        "\n",
        "    # Tokenize English translations\n",
        "    inputs = tokenizer(en_texts, truncation=True, padding=\"max_length\", max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "    # Tokenize Russian translations\n",
        "    targets = tokenizer(ru_texts, truncation=True, padding=\"max_length\", max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "    # Add the Russian tokens as labels\n",
        "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
        "\n",
        "    return inputs\n",
        "\n",
        "from transformers import MarianMTModel, MarianTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "\n",
        "import transformers\n",
        "print(transformers.__version__)\n",
        "\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",          # Where to save the model and logs\n",
        "    num_train_epochs=3,              # Number of training epochs\n",
        "    per_device_train_batch_size=8,   # Batch size for training\n",
        "    per_device_eval_batch_size=8,    # Batch size for evaluation\n",
        "    logging_dir=\"./logs\",            # Directory for logs\n",
        "    logging_steps=500,               # Log every 500 steps\n",
        "    save_steps=1000,                 # Save model every 1000 steps\n",
        "    load_best_model_at_end=False,    # Disable loading the best model\n",
        "    weight_decay=0.01,               # Weight decay\n",
        ")\n",
        "\n",
        "\n",
        "from transformers import MarianMTModel, MarianTokenizer, pipeline\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model_name = \"Helsinki-NLP/opus-mt-en-ru\"\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "model = MarianMTModel.from_pretrained(model_name)\n",
        "\n",
        "# Create a translation pipeline\n",
        "translator = pipeline(\"translation_en_to_ru\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Example English sentences\n",
        "english_sentences = [\"Hi, My name is Nemat Dadashov\"]\n",
        "\n",
        "\n",
        "\n",
        "# Translate\n",
        "russian_translations = translator(english_sentences)\n",
        "\n",
        "# Display the results\n",
        "for en, ru in zip(english_sentences, russian_translations):\n",
        "    print(f\"ENGLISH: {en}\")\n",
        "    print(f\"RUSSIAN: {ru['translation_text']}\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}